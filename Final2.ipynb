{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aD5aZYA3kuV",
        "outputId": "81837d13-0ec3-4315-9b1e-ae39fad5f4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 60, 1, 8)          664       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 58, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 29, 1, 8)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 27, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 13, 1, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 104)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 104)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 105       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "400/400 [==============================] - 65s 160ms/step - loss: 0.4293 - acc: 0.5794 - f1macro: 0.4208 - val_loss: 0.4949 - val_acc: 0.4992 - val_f1macro: 0.3700\n",
            "Epoch 2/20\n",
            "400/400 [==============================] - 63s 157ms/step - loss: 0.3047 - acc: 0.7193 - f1macro: 0.6912 - val_loss: 0.4698 - val_acc: 0.5273 - val_f1macro: 0.4969\n",
            "Epoch 3/20\n",
            "400/400 [==============================] - 61s 154ms/step - loss: 0.2323 - acc: 0.7864 - f1macro: 0.7737 - val_loss: 0.4678 - val_acc: 0.5273 - val_f1macro: 0.5125\n",
            "Epoch 4/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.2085 - acc: 0.8045 - f1macro: 0.7940 - val_loss: 0.4405 - val_acc: 0.5609 - val_f1macro: 0.5356\n",
            "Epoch 5/20\n",
            "400/400 [==============================] - 63s 157ms/step - loss: 0.1950 - acc: 0.8148 - f1macro: 0.8058 - val_loss: 0.4509 - val_acc: 0.5484 - val_f1macro: 0.5355\n",
            "Epoch 6/20\n",
            "400/400 [==============================] - 62s 155ms/step - loss: 0.1853 - acc: 0.8239 - f1macro: 0.8154 - val_loss: 0.4600 - val_acc: 0.5352 - val_f1macro: 0.5237\n",
            "Epoch 7/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1768 - acc: 0.8311 - f1macro: 0.8231 - val_loss: 0.4818 - val_acc: 0.5172 - val_f1macro: 0.5024\n",
            "Epoch 8/20\n",
            "400/400 [==============================] - 61s 154ms/step - loss: 0.1699 - acc: 0.8371 - f1macro: 0.8293 - val_loss: 0.4756 - val_acc: 0.5258 - val_f1macro: 0.5114\n",
            "Epoch 9/20\n",
            "400/400 [==============================] - 63s 157ms/step - loss: 0.1704 - acc: 0.8356 - f1macro: 0.8279 - val_loss: 0.4783 - val_acc: 0.5203 - val_f1macro: 0.5055\n",
            "Epoch 10/20\n",
            "400/400 [==============================] - 63s 157ms/step - loss: 0.1674 - acc: 0.8381 - f1macro: 0.8305 - val_loss: 0.4612 - val_acc: 0.5336 - val_f1macro: 0.5270\n",
            "Epoch 11/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1625 - acc: 0.8423 - f1macro: 0.8345 - val_loss: 0.4749 - val_acc: 0.5242 - val_f1macro: 0.5117\n",
            "Epoch 12/20\n",
            "400/400 [==============================] - 61s 154ms/step - loss: 0.1611 - acc: 0.8433 - f1macro: 0.8359 - val_loss: 0.4877 - val_acc: 0.4992 - val_f1macro: 0.4917\n",
            "Epoch 13/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1597 - acc: 0.8449 - f1macro: 0.8384 - val_loss: 0.4797 - val_acc: 0.5195 - val_f1macro: 0.5084\n",
            "Epoch 14/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1587 - acc: 0.8461 - f1macro: 0.8396 - val_loss: 0.4652 - val_acc: 0.5391 - val_f1macro: 0.5246\n",
            "Epoch 15/20\n",
            "400/400 [==============================] - 61s 154ms/step - loss: 0.1548 - acc: 0.8499 - f1macro: 0.8436 - val_loss: 0.4819 - val_acc: 0.5211 - val_f1macro: 0.5105\n",
            "Epoch 16/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1535 - acc: 0.8502 - f1macro: 0.8445 - val_loss: 0.4613 - val_acc: 0.5367 - val_f1macro: 0.5264\n",
            "Epoch 17/20\n",
            "400/400 [==============================] - 61s 154ms/step - loss: 0.1521 - acc: 0.8518 - f1macro: 0.8458 - val_loss: 0.4797 - val_acc: 0.5172 - val_f1macro: 0.5099\n",
            "Epoch 18/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1470 - acc: 0.8569 - f1macro: 0.8512 - val_loss: 0.5133 - val_acc: 0.4828 - val_f1macro: 0.4687\n",
            "Epoch 19/20\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1469 - acc: 0.8568 - f1macro: 0.8509 - val_loss: 0.4839 - val_acc: 0.5164 - val_f1macro: 0.5080\n",
            "Epoch 20/20\n",
            "400/400 [==============================] - 61s 153ms/step - loss: 0.1472 - acc: 0.8559 - f1macro: 0.8501 - val_loss: 0.4836 - val_acc: 0.5234 - val_f1macro: 0.5141\n",
            "accuracy: 0.5004878048780488\n",
            "MAE: 0.4995121951219512\n",
            "F1: 0.5477031802120141\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        " \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
        " \n",
        "DATADIR = \"/content/drive/MyDrive/colab drives\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        " \n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        " \n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        " \n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        " \n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        " \n",
        "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        " \n",
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)     # pick one time step\n",
        "            n = (df.index == t).argmax() # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X, y\n",
        "            batch = []\n",
        " \n",
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        # extract sample using a sliding window\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    return np.expand_dims(np.array(X),3), np.array(y)\n",
        " \n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir(DATADIR):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join(DATADIR, filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        " \n",
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 20\n",
        "n_features = 82\n",
        " \n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_2d(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        " \n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
        " \n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        " \n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ]
    }
  ]
}